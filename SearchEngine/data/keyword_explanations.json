{
    "cnn": "Convolutional Neural Network, a type of deep neural network used primarily for image recognition.",
    "lstm": "Long Short-Term Memory, a type of recurrent neural network used for sequence prediction.",
    "transformer": "A neural network architecture that relies on self-attention mechanisms, often used in natural language processing.",
    "rnn": "Recurrent Neural Network, a type of neural network where connections between nodes can create cycles.",
    "svm": "Support Vector Machine, a supervised learning model used for classification and regression analysis.",
    "xgboost": "Extreme Gradient Boosting, a scalable machine learning system for tree boosting.",
    "random forest": "An ensemble learning method for classification, regression, and other tasks that operates by constructing a multitude of decision trees.",
    "knn": "K-Nearest Neighbors, a simple, instance-based learning algorithm used for classification and regression.",
    "pca": "Principal Component Analysis, a technique used to emphasize variation and bring out strong patterns in a dataset.",
    "autoencoder": "A type of artificial neural network used to learn efficient codings of unlabeled data.",
    "gan": "Generative Adversarial Network, a class of machine learning frameworks designed by two neural networks competing against each other.",
    "vae": "Variational Autoencoder, a type of autoencoder that introduces a probabilistic twist.",
    "deep learning": "A subset of machine learning where neural networks with many layers learn from vast amounts of data.",
    "image segmentation": "The process of partitioning a digital image into multiple segments to simplify or change the representation of an image.",
    "object detection": "A computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class in digital images and videos.",
    "feature extraction": "The process of transforming raw data into a set of features that can be used in machine learning models.",
    "dimensionality reduction": "The process of reducing the number of random variables under consideration by obtaining a set of principal variables.",
    "neural network": "A series of algorithms that attempts to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.",
    "medical imaging": "Techniques and processes used to create images of various parts of the human body for diagnostic and treatment purposes.",
    "radiomics": "The extraction of a large number of features from radiographic medical images using data-characterization algorithms.",
    "supervised learning": "A type of machine learning where the model is trained on labeled data.",
    "unsupervised learning": "A type of machine learning where the model is trained on unlabeled data.",
    "semi-supervised learning": "A type of machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training.",
    "reinforcement learning": "A type of machine learning where an agent learns to make decisions by performing actions and receiving rewards.",
    "overfitting": "A modeling error in machine learning when a function is too closely fit to a limited set of data points.",
    "underfitting": "A modeling error in machine learning when a function is too simple to capture the underlying structure of the data.",
    "cross-validation": "A technique for assessing how the results of a statistical analysis will generalize to an independent data set.",
    "hyperparameter tuning": "The process of choosing a set of optimal hyperparameters for a learning algorithm.",
    "gradient descent": "An optimization algorithm used to minimize the cost function in machine learning models.",
    "backpropagation": "An algorithm used to train neural networks, by updating weights in the network to reduce the error.",
    "epoch": "A single pass through the entire training dataset.",
    "batch size": "The number of training examples utilized in one iteration.",
    "dropout": "A regularization technique for reducing overfitting in neural networks by randomly dropping units during training.",
    "convolution": "A mathematical operation used in CNNs to extract features from data.",
    "max pooling": "A down-sampling technique in CNNs that reduces the dimensionality of input data.",
    "flattening": "A process in CNNs where the matrix is converted into a one-dimensional array.",
    "fully connected layer": "A layer in neural networks where each neuron is connected to every neuron in the previous layer.",
    "activation function": "A function used in neural networks to introduce non-linearity.",
    "softmax": "An activation function used in neural networks to convert the output into a probability distribution.",
    "relu": "Rectified Linear Unit, an activation function used in neural networks.",
    "sigmoid": "An activation function used in neural networks that outputs a value between 0 and 1.",
    "tanh": "Hyperbolic Tangent, an activation function used in neural networks.",
    "learning rate": "A hyperparameter that controls how much to change the model in response to the estimated error.",
    "momentum": "A technique to accelerate gradient descent by adding a fraction of the previous update to the current update.",
    "batch normalization": "A technique to improve the training of deep neural networks by normalizing the inputs of each layer.",
    "data augmentation": "A technique to increase the diversity of training data by applying random transformations.",
    "transfer learning": "A machine learning technique where a model trained on one task is re-purposed on a second related task.",
    "fine-tuning": "A process of training a pre-trained model on a new dataset.",
    "pretraining": "Training a model on a large dataset before fine-tuning it on a specific task.",
    "one-hot encoding": "A process to convert categorical data into binary vectors.",
    "embedding": "A learned representation for text or other categorical data.",
    "tokenization": "The process of splitting text into smaller units such as words or subwords.",
    "attention mechanism": "A technique in neural networks to focus on relevant parts of the input sequence.",
    "beam search": "An optimization algorithm that explores a graph by expanding the most promising nodes.",
    "BLEU score": "A metric for evaluating a generated sentence to a reference sentence."
}
